[{"title":"特征平台调研-网易新闻推荐工程","date":"2023-02-28T03:38:18.000Z","path":"2023/02/28/特征平台调研-网易新闻推荐工程/","text":"业务特性 信息流推荐，实时性要求高 例如图中的用户实时兴趣特征(受到用户兴趣迁移用户反馈等影响)，文章的实时点击率等 image.png 历史版本 常规的做法离线训练时将日志表、用户表、文章表等离线表 join 后产生样本，在线服务有专门的服务加速从数据表中获取内容的过程。 注：这种方式和我们目前在用的方式差不多，离线训练使用 hive 表关联，并将特征分组后存储到 redis 中，在线服务时直接从 redis 中获取特征，这种方式基本不涉及到用户或物料维度的实时特征信息 网易新闻历史版本的基本思路是： 线上服务实时生产特征，离线样本生成使用线上实时落盘的特征 image.png 流程： 在线服务阶段在精排之前，获取或计算用户侧、文章侧的各个维度原始属性并人工特征工程以得到结构化、规范化的特征数据，以此特征进行预测 完成精排后，将此刻预估时的特征数据落盘 离线训练阶段，使用落盘的特征数据，以用户 ID 和文章 ID 为 key，与曝光日志、点击日志、其他行为日志关联标签，形成带有标签的特征样本 利弊： 可以保证线上和训练的特征一致性，避免特征穿越问题。 灵活性差，且对存储有较高的要求 设计方案 优化灵活性 抽象出特征算子 比如bucket、cross、cosine，基于json文件做特征计算管理，线上和线下基于同样的特征算子包做计算 将特征区分为原始特征和特征，比如用户年龄 27 岁为物料，而年龄 3 号桶则为特征，日志落盘的是物料数据，在线和离线使用的是同一套特征算子所以可以保证一致性。注： 我理解这边主要是落盘元数据，可以通过物料计算的特征数据都不需要落盘，这样一方面可以保证性能，一方面可以减少存储的需求。 基本流程如下图所示 算法同学主要的工作集中在特征工程和模型训练上，算法人员可以根据提供的基础工具和基础特征算子书写特征配置 优化业务复用性 每个子业务都需要构架上述的服务, 为了解决业务复用问题，引入物料服务和物料缓存网关的概念 image.png 离线环节：各个业务依然是独自落原始物料，经过特征算子计算生成样本，没有改变； 在线环节：pCTR 服务不再直接和物料服务通信，而是借助物料缓存网关。物料缓存网关的核心功能则为缓存和路由，其根据特征名称路由到不同的物料服务来计算 注：我理解这边就是由业务场景构建属于业务自己的物料库，即对不同业务的物料信息做隔离，由物料缓存网关做路由 优化性能为了灵活性，我们引入了特征算子包，在离线和线上做实时的计算。然而实时计算就意味着 RT 增加，随着进入 pCTR 排序服务的候选数目的扩大，RT 的压力越来越大（需要在线做特征计算，CPU 密集型操作,召回数量增加直接导致 RT 升高)优化方向 这边可以学习下问题分析的思路 算子库实现的性能优化 pCTR 服务端的性能优化 特征平台侧的优化降低 RT 的常见思路为用空间换时间，给定特征配置文件，同个用户的不同请求中，类似年龄性别等人口属性和用户的长期兴趣是固定的，没有必要每次都计算。将某些类型的特征提前计算好并存储到缓存中，线上请求直接拉取对应的特征即可 image.png 物料和特征生产：触发物料服务计算有消息队列和接口调用两种方式，物料服务除了计算物料之外，还会将具体的特征也计算完成，并且将结果分别缓存 线上服务的获取：线上服务的获取只是会区分物料和特征。针对精排服务而言，其直接获取到特征粒度，物料缓存服务网关需要根据其对应的特征配置拉取具体的特征。而在落物料日志的过程中，则获取物料粒度，这样在性能优化的同时依旧保持了灵活性 样本生成逻辑 将样本生成的过程拆分为两个环节：join 环节、sample 环节。join 环节的核心工作为融合信息，融合如物料信息、推荐点击信息、用户互动信息等等，同时我们将业务中常见的标签推理的方式封装为函数，外面直接设置相关的参数即可得到实际的标签，比如加上阅读时长、阅读完成度类似的限制。sample 环节的工作为特征工程，除了基于配置的特征工程之外，我们将样本权重计算、样本过滤逻辑、样本采样逻辑也做了函数式的封装，并对外暴露参数使其可配置 样本生成逻辑是基于 flink 任务来完成的，flink 任务会定期读取数据表，获取需要计算的样本详情，生成新的样本 image.png 特征算子 一套基于特征配置的特征算子库，以解决现有特征工程方法中存在的特征工程策略可读性差、通用性差、特征调整迭代周期长、计算冗余度高等问题，能够在灵活性、高效性、复用性上有很好的表现能力。 对训练样本需要的特征进行汇总分析，并对依赖的原始数据进行梳理，整理出由原始数据到特征样本之间所有的变换、组合关系，我们称之为特征算子 特征配置设计 json格式，支持嵌套。例如{“feat”: “a &#x3D; op(b)”, “param_A”: “val_A”, “paramB”: “val_B”}，表示使用op对原始数据b变换处理得到特征a，变换过程中需要两个参数 参考资料[1] 网易新闻推荐工程优化 - 特征平台篇 - AIQ[2] 网易新闻推荐工程优化 - 特征算子篇 - AIQ","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://xiaoysec.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"特征平台","slug":"特征平台","permalink":"https://xiaoysec.github.io/tags/%E7%89%B9%E5%BE%81%E5%B9%B3%E5%8F%B0/"},{"name":"系统设计","slug":"系统设计","permalink":"https://xiaoysec.github.io/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"}]},{"title":"推荐工程架构方案调研","date":"2023-02-20T07:50:29.000Z","path":"2023/02/20/推荐工程架构方案调研/","text":"调研背景和目的目前公司的推荐系统已经初步形成召回-精排-重排的架构，但在一些基础设施上还有诸多的不足，这次调研旨在借鉴外部公司优秀成熟的推荐工程架构方案，寻找当前最适合公司的系统设计方案，进一步优化工程链路。 闲鱼推荐系统调研淘系推荐系统多基于 BE、RTP 引擎实现业务的定制化，比较常见的如手淘首猜、手猫首猜等场景。而闲鱼推荐系统有一定的业务特殊性，由于是二手交易所以商品多为“孤品”，对实时性要求也相对较高。 整体架构推荐系统流程和涉及模块如下图所示 image.png 用户信息： 通过用户 id 从用户中心服务获取用户的基本信息 Trigger: 一般为用户历史浏览、点击、互动行为的商品，或者用户的属性等。通过数据回流将 trigger 信息存储在在线系统，通过用户 ID 查询出来，参与后续的流程 召回: 基于用户的特征和 trigger 信息，从全量的亿级别候选集中，挑选出万级别的商品，送入后续的算分排序阶段 粗&#x2F;精排: 粗排一般使用简单网络结构如双塔，挑选千级别进精排。精排阶段会使用复杂模型 重排: 重排阶段，则会体现场景的业务诉求，做一下打散干预，比如类目打散、价格打散等 结果返回: 选取 topK 个商品，一般 10-20 个，填充商品信息后返回调用方 日志&amp;埋点: 对展示给用户的商品做端上日志 在业务发展的过程中也存在电商推荐普遍存在的问题，如推荐场景的增加带来的迭代开发和维护成本增加，应对这种情况，通常需要工程团队抽象链路组件和模块，实现场景的配置化，下面展示了闲鱼推荐链路开发工作流。 image.png 整个闲鱼的推荐平台如下图所示，大致上包含了 1. 底池圈品 2. 算法离线训练 3.引擎层(BE、RTP 之类) 4. 应用服务(类似 TPP 应用层) ，当然还会有其他外围系统如 AB 实验系统、KM 监控等等。 image.png 下图展示了用户请求后的完整链路注： 对比下我们的链路，最大的区别是特征读取环节和 summary 环节以及重排阶段。在闲鱼的召回中充分利用到用户的实时行为，而我们的召回多以离线方式产出，在线召回阶段不需要复杂的用户特征了；另一方面 summary 这一步我们算法服务不涉及，由调用方(导购为主) 补全商品信息；重排阶段我们是放在了精排中作为一个模块，算法管理后台配置决定是否重排和重排的方式。 image.png 圈品逻辑推荐候选池: 即圈品构建推荐底池，ETL 后在商品上打上场景标，圈品的逻辑在体量大一些的公司都存在，一方面是业务逻辑需要，另一方面是缩小推荐量级。下图展示了推荐候选池的构建过程，整体应该是基于 SARO 平台完成的。 image.png 召回引擎召回引擎: 提供了三种索引方式，分别是 i2i 索引，x2i 索引，深度召回，标准化输入与输出。 image.png 对于输入，第一种为 Trigger 格式，引擎将以传入的 Trigger 作为 key，从 i2i 与 x2i 索引中执行 KV 检索与倒排检索。第二种则是针对深度召回，上游传入模型预测出的 embedding 向量，再经由向量引擎完成检索。输出则是召回检索得到的商品 item_id 与对应的召回 recall_score。 i2i 召回，通过用户的行为日志，计算商品间的用户共现得分，以此为相似度。以用户有过行为的商品 ID 为 TriggerKey 查询召回商品。 image.png x2i: x 可以是商品的 tag、class、brand、query、pool_ids 等，根据用户全域的行为构建用户偏好，对商品标题信息进行分词，以及用户的 tag，class，品牌，搜索场景下对应 query 等，最终构建倒排索引进行检索 image.png 可以看到 i2i 和 x2i 召回在存储上有所不同，i2i 使用了 KKV 结构而 x2i 使用了倒排，应该是基于性能的考量，i2i 不需要复杂查询 深度召回：主要通过深度网络模型，来预测用户与商品的相似性。模型分别计算出用户侧向量与商品侧向量，在线检索时，根据用户侧向量，通过向量引擎完成 ANN 检索出 topK 个商品。 image.png 算分引擎算分引擎将输入的待打分候选商品集关联上商品特征，并结合用户的特征，通过深度网络模型的计算执行，完成候选商品集中每一个商品对该用户的个性化预测得分。将算分排序模型的输入输出进行标准化，也提供了模型定制化的能力。有些场景不太适应通用的多目标模型，可遵循协议将模型接入，每一个模型具备一个唯一的标识 biz_name，场景配置上选择该 biz_name 即可 image.png 模型存在多个目标得分，比如 ctr_score、cvr_score、car_score 等。而最终的得分如何计算，场景内也支持配置运算表达式与加权&amp;降权（有些场景倾向转化，有些场景则重成交，或者满足交易抵扣的商品需要提权），来满足不同的场景要求。注： 这种配置化权重的做法在我们精排服务中也支持(ESMM 模型)，只是整体上没有闲鱼做的这么标准化 image.png 贝壳推荐系统调研贝壳的文章中提到了推荐系统在贝壳的演进过程，同时讲到了搜索推荐架构融合的话题，比较有启发性。 推荐系统演进过程架构演进经历了四个大的迭代，最早期就是简单的基于内容和规则的推荐引擎，后面进一步增加了用户画像和协同过滤进行个性化推荐，再通过实时计算和实时模型实现实时个性化推荐，最后为了提升业务接入和迭代效率，推荐平台做了一个大的升级重构，支持业务配置化接入，最终升级为智能推荐平台。 基于内容的推荐 用 Content-based 推荐算法，直接离线算出相似房源、热门房源等，然后写入 Redis，在线推荐服务再从 Redis 中查出离线计算好的可能感兴趣的房源，然后直接返回给用户进行推荐 image.png 实时个性化推荐 在内容推荐的基础上，引入房源特征、实时用户画像和实时用户行为记录，升级为实时个性化推荐。个性化推荐底层新增经纪人作业数据、用户行为日志等数据，然后通过离线计算进行数据清洗和特征工程，生成房源特征和用户画像。再通过协同过滤算法，进行协同过滤推荐，然后把这些数据批量更新到在线存储引擎，包括离线计算好的召回数据、特征池和过滤集等，业务系统和推荐服务都会将实时埋点日志回流到实时计算服务和离线数仓中。从而实时更新召回数据和特征实现实时个性化推荐 image.png 智能平台推荐 为了提升业务接入效率和效果迭代效率，不再每个业务场景对应一个独立的推荐服务，而是用同一套推荐服务支撑上层的所有业务，新接业务直接复用上线，而非重新开发启动一个服务。将整个推荐服务做逻辑分层，分为应用层、计算层、数据层和模型层。应用层主要对外提供 API 接口，以及处理简单的业务规则和配置管理。计算层包含推荐的几个核心流程，如召回、融合、排序和过滤，会分别调用数据层和模型层。数据层统一对下层的在线存储系统进行基础的数据查询。模型层进行在线特征工程后会调用模型服务进行在线预测。计算层拿到数据层返回的结果后进行策略融合，然后调用模型层进行模型精排，最终返回给业务系统。注：本质上也是接入层-召回层-排序层的抽象，和我们的做法基本上是一致的 搜索推荐架构统一搜索和推荐有很多相通之处，在流程上基本非常相似，在数据和模型上有多可以复用，在外围平台工具层面也基本可以直接复用，不同之处主要是 Query，搜索围绕 Query 展开，而推荐没有明显的用户意图引导，以用户历史行为作为参考。 image.png 为什么要搜推架构统一: 核心是复用底层能力，避免重复建设带来的人力浪费和开发运维复杂性架构统一方案架构图如下所示 image.png 我们做的主要的大的重构是在查询层，对原搜索和推荐系统的各模块进行了统一的整合。最新的查询层主要分为六个核心模块，请求一开始会通过中控模块做参数校验、策略调度、缓存和兜底，然后中控会去调用下层各模块，先是意图解析模块（搜索使用，推荐不需要），拿到意图解析的结果后再去调用召回模块，召回的时候会先获取一些用户画像和特征，然后进行多路召回和融合过滤，返回给中控，中控得到召回的数据后调用排序，排序包括粗排和精排，接下来是重排，再之后调用理由模块，补充推荐理由，比如“满五唯一”，“近地铁”等等。拿到理由之后就会最终反馈给业务方，完成整个搜索推荐调用的过程 中控服务 中控主要负责参数校验、调度、缓存、降级等功能，中控服务的设计原则是希望它尽量不要有业务逻辑，通过减少迭代最大化的保证中控服务的稳定性注: 这一点有点类似阿里搜索用的 SP，但是贝壳的中控应该更“薄”，和我们召回引擎中的模块编排也是相同的思想，同时贝壳这个超时降级的方案也非常值得学习最后中控最大的作用就是降级，任何下游服务超时或异常都不会造成业务方的查询异常，各个模块都有默认超时时间设置，但同时会实时计算剩余时间，各模块的实际超时时间是该模块默认超时时间和剩余时间的最小值。比如一个常规的调用链，开始调用意图解析，再调用召回，再反馈给业务方。假设我们调完重排要调用理由的时候，发现理由服务挂掉或者响应超时，中控则会跳过理由模块直接返回，相当于是降级返回。如果召回模块超时，中控也会跳过召回模块，直接访问 ES 或 Redis，然后再拿这些结果去走后续的流程，相当于跳过整个召回逻辑直接拿基础引擎返回的召回数据传给排序走后面的流程。比如在异常情况下我们调重排的时候发现已经花了 950ms，由于只剩下 50ms，所以再去调理由的时候，理由模块的超时时间会被实时设置为 50ms，而忽略其默认的超时时间。 召回服务 image.png 重排服务 为了便利的组合复用这些规则逻辑，重排实现了 workflow 的工作流机制。例如默认配置中会有去重、融合、计算得分、按字段排序等默认规则，而“opt-in”可以增加规则，“opt-out”可以去除规则 image.png 注：相对比较重，我们还是目前的重排还是多以策略打散为主，也有一些业务规则上的重排但是不太好做到标准化。 总结现有架构以算法管理后台为中心，召回引擎、精排引擎作为底层基础设施，对外提供商品(内容)推荐、搜索排序、类目预测、个性化分发等服务。以推荐服务为例，目前我们也做到了召回-精排等主要服务的解耦，可以实现多场景动态化配置，使得新的推荐场景的接入成本进一步降低。 主要问题当前的系统架构还存在许多不足和不合理的地方，最主要的有以下几点 系统耦合 业务初期算法底层支撑系统建设有所欠缺，导致部分业务系统需要自行完成召回、调用精排等操作，系统之间耦合程度比较高。独立出统一召回系统后，一定程度上降低了系统间的耦合性。但目前业务系统和算法服务系统之间仍有不少可改进之处，如业务系统通过场景 ID 获取精排配置后调用精排服务。理想情况下，业务系统只关心场景 ID，具体的召回、精排模型等配置对业务系统完全透明。 特征管理 当前针对每一个精排模型，算法同学需要手动维护多组特征，一方面增加了算法同学的工作量，另一方面也增加了存储的成本。而一些大公司普遍会自研特征平台，实现离在线特征的存储和用户、商品特征的管理，为召回引擎、搜索引擎、精排引擎等提供基础底层服务。 实时性 实时性主要体现在两方面，对用户行为的利用和商品元数据的增量更新。在用户行为利用层面，以召回方式为例子，目前公司以离线召回为主，如双塔召回模型通过离线训练直接产出每个门店的召回结果，而更常见的做法是先在线通过模型计算用户维度向量再通过向量检索引擎获取召回物料；又如协同过滤召回中，目前我们会在离线阶段关联用户-商品行为表和商品-商品相关性表产出召回结果，而一些公司会通过在线方式获取用户一段时间内有过行为的商品作为 Trigger，再通过 Trigger 查询 i2i 表获取召回结果。上述两个例子都是将用户实时行为利用到召回中，对召回结果实时性有一定的作用。在商品元数据更新层面，目前我们的链路中还不存在索引服务，也不存在增量消息更新。如发布端对商品基础信息做修改或商品售罄等情况，推荐侧是无法感知的，这对推荐结果的实时性有一定的影响。 链路闭环 一条完整的推荐链路涉及离线数据分析处理、样本采集、模型训练、索引构建、召回&amp;精排引擎部署、在线服务开发、埋点及端日志采集、AB 效果分析等。埋点和日志采集涉及客户端配合，往往是比较容易出问题的一环，且一旦有问题也不容易被发现。目前我们分析实验效果需要关联离线报表（商品曝光+AB 平台日志，通过门店 ID 关联），因此如果对实验效果实时性有要求是无法满足的。一些公司会在服务端将实验信息添加到埋点中，客户端透传上报，这种方式更加灵活且避免了实验轮转带来的数据不准确问题，同时借助流计算可以做到实验的实时效果分析和业务指标的及时告警。 参考文档[1] 详解闲鱼推荐系统（长文收藏） - 掘金[2] BasicEngine — 基于 DII 平台的推荐召回引擎-阿里云开发者社区[3] 如何快速拉起多路召回服务_智能推荐 AIRec-阿里云帮助中心[4] 调用 BeRead 接口获取智能召回引擎的召回结果_智能推荐 AIRec-阿里云帮助中心[5] 当你打开天猫的那一刻，推荐系统做了哪些工作？AI陈启伟_InfoQ 精选文章[6] 贝壳找房 | 降本提效，贝壳搜索推荐架构统一之路 - AIQ","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://xiaoysec.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"架构设计","slug":"架构设计","permalink":"https://xiaoysec.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}]},{"title":"语雀文章同步Hexo","date":"2023-02-20T05:34:21.000Z","path":"2023/02/20/语雀文章同步Hexo/","text":"前提条件搭建好 Hexo 博客环境 配置步骤插件源码地址 https://github.com/x-cold/yuque-hexo 安装 npm install -g yuque-hexo –save 配置 修改博客根目录下的 package.json 文件 12345678910111213141516171819\"yuqueConfig\": &#123; \"postPath\": \"source/_posts\", \"cachePath\": \"yuque.json\", \"mdNameFormat\": \"slug\", \"adapter\": \"hexo\", \"concurrency\": 5, \"baseUrl\": \"https://www.yuque.com/api/v2\", \"login\": \"xituchengxiaoyang\", \"repo\": \"blog\", \"token\": \"语雀token\", \"onlyPublished\": true, \"onlyPublic\": true&#125;,\"devDependencies\": &#123; \"yuque-hexo\": \"^1.6.0\"&#125;,\"hexo\": &#123; \"version\": \"4.2.1\"&#125;, 参数解释 参数名 含义 默认值 postPath 文档同步后生成的路径 source&#x2F;_posts&#x2F;yuque cachePath 文档下载缓存文件 yuque.json mdNameFormat 文件名命名方式 (title &#x2F; slug) title adapter 文档生成格式 (hexo&#x2F;markdown) hexo concurrency 下载文章并发数 5 baseUrl 语雀 API 地址 - login 语雀 login (group), 也称为个人路径 - repo 语雀仓库短名称，也称为语雀知识库路径 - onlyPublished 只展示已经发布的文章 false onlyPublic 只展示公开文章 false baseUrl 是固定的照抄就行，mdNameFormat 建议使用 title，repo 信息在这边看 image.png Token 需要设置一下 image.png 在 package.json 中配置 scripts 12345//添加以下命令行&#123; \"sync\": \"yuque-hexo sync\", \"clean:yuque\": \"yuque-hexo clean\"&#125; 完整的 package.json 文件如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; \"name\": \"hexo-site\", \"version\": \"0.0.0\", \"private\": true, \"scripts\": &#123; \"build\": \"hexo generate\", \"clean\": \"hexo clean\", \"deploy\": \"hexo deploy\", \"server\": \"hexo server\", \"sync\": \"yuque-hexo sync\", \"clean:yuque\": \"yuque-hexo clean\" &#125;, \"hexo\": &#123; \"version\": \"4.2.1\" &#125;, \"dependencies\": &#123; \"hexo\": \"^4.2.1\", \"hexo-deployer-git\": \"^4.0.0\", \"hexo-generator-archive\": \"^2.0.0\", \"hexo-generator-category\": \"^2.0.0\", \"hexo-generator-feed\": \"^3.0.0\", \"hexo-generator-index\": \"^3.0.0\", \"hexo-generator-json-content\": \"^4.2.3\", \"hexo-generator-tag\": \"^2.0.0\", \"hexo-helper-qrcode\": \"^1.0.2\", \"hexo-renderer-ejs\": \"^2.0.0\", \"hexo-renderer-less\": \"^4.0.0\", \"hexo-renderer-marked\": \"^6.0.0\", \"hexo-renderer-stylus\": \"^2.1.0\", \"hexo-server\": \"^3.0.0\", \"hexo-theme-landscape\": \"^0.0.3\", \"yuque-hexo\": \"^1.9.5\" &#125;, \"devDependencies\": &#123; \"yuque-hexo\": \"^1.9.5\" &#125;, \"yuqueConfig\": &#123; \"postPath\": \"source/_posts/yuque\", \"cachePath\": \"yuque.json\", \"mdNameFormat\": \"title\", \"adapter\": \"hexo\", \"concurrency\": 5, \"baseUrl\": \"https://www.yuque.com/api/v2\", \"login\": \"XXXXX\", //你自己的用户名 \"repo\": \"XXXX\", //你自己的仓库名 \"token\": \"XXXXX\", //你自己的token \"onlyPublished\": false, \"onlyPublic\": false &#125;&#125; 同步命令 yuque-hexo sync 同步yuque-hexo clean 清楚本地缓存 一些坑 语雀图片无法正常显示 因为语雀图片防盗链机制，图片无法正常显示，需要对主题下的文件做一点修改如我使用的 indigo，需要修改 themes&#x2F;indigo&#x2F;layout&#x2F;_partial&#x2F;head.ejs ，添加如下的源码 1&lt;meta name=\"referrer\" content=\"no-referrer\" /&gt; 部署出错，kex_exchange_identification: Connection closed by remote github 22 端口 timeout,直接使用 443 端口解决，在**~&#x2F;.ssh&#x2F;config **文件中添加如下 1234Host github.comHostName ssh.github.comUser gitPort 443 在命令行工具中使用代理 在.zshrc 或者.bash_profile 中添加如下命令，后重新加载环境变量，需要使用到梯子只需要先执行 on_proxy 123456789101112131415function on_proxy() &#123; export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com\" export http_proxy=\"http://127.0.0.1:9850\" export https_proxy=$http_proxy export all_proxy=socks5://127.0.0.1:9850 echo -e \"\\n\" echo -e \"proxy is on\"&#125;function off_proxy()&#123; unset http_proxy unset https_proxy unset all_proxy echo -e \"proxy is off\"&#125; mac 下查看 clash 中的代理信息如下 image.png 不要使用 ping www.google.com来进行测试，因为ping命令使用的是ICMP协议，是不支持代理的。 参考文章[1] http://www.manongjc.com/detail/61-xmojtxvddsuftbg.html[2] https://cloud.tencent.com/developer/article/2114329[3] https://cloud.tencent.com/developer/article/2168702[4] https://zhuanlan.zhihu.com/p/577256660","tags":[{"name":"效率工具","slug":"效率工具","permalink":"https://xiaoysec.github.io/tags/%E6%95%88%E7%8E%87%E5%B7%A5%E5%85%B7/"},{"name":"Hexo","slug":"Hexo","permalink":"https://xiaoysec.github.io/tags/Hexo/"}]},{"title":"召回系统在海拍客的实践","date":"2023-02-20T01:51:17.000Z","path":"2023/02/20/召回系统在海拍客的实践/","text":"前言推荐系统通常可以分为召回、粗排、精排、重排四个阶段，召回主要是根据不同策略或模型从海量的物品库中快速筛选出一小部分用户可能感兴趣的物品，交由排序模型来精准地完成个性化排序，本文主要阐述召回在算法侧的工作原理以及召回系统工程侧落地实践。 召回算法目前算法团队在使用传统算法如 ItemCF、Swing 的基础上，也在不断尝试在召回中应用深度模型，下面章节介绍目前算法团队使用到的几种典型召回算法原理，实际的模型会更复杂但原理大同小异。 协同过滤作为在推荐系统召回中最基本的一种算法，系统过滤主要分为两大类 基于用户的协同过滤算法(UserCF) 基于物品的协同过滤算法(ItemCF) 基于用户的协同过滤简单的说基于用户的协同过滤就是找到与你相似的用户，将相似用户交互过的商品推荐给你。基于用户的协同过滤核心在于计算用户的相似度，以一个简单的例子演示该算法假设我们在做首页商品推荐，用户 A 点击的商品集合为 M，用户 B 点击的商品集合为 N,那么用户 A 和用户 B 的相似度可以通过如下公式计算 image.png 如用户 A 点击过的商品集合 M &#x3D; {a,b,c,d} , 用户 B 点击过的商品集合 N&#x3D;{a,b,c,d,e,f} ，所以用户 A 和用户 B 的相似度为 2&#x2F;3，用户 C 点击过的商品集合为{a,b,c} ，用户 D 点击过的商品集合为{a,b}，此处我们人为定义相似度阈值为 0.5，那么与用户 A 相似的用户为用户 B 和用户 C，那么可以给用户 A 推荐商品{e,f}，给用户 C 推荐{d,e,f}。 这里只是展示了一种比较简单粗暴的方法，想要得到更好的推荐效果可以改进相似度的计算方法，这边不作深究。 基于物品的协同过滤简单来说基于物品的协同过滤就是根据用户对商品的交互历史，推荐该商品相似的其他商品。基于物品的协同过滤同样需要计算物品的相似度，以一个简单的例子演示该算法假设喜欢物品 a 的用户数量为 N(a)，喜欢物品 b 的用户数量为 N(b) ，那么物品 a 和物品 b 的相似度可以使用如下公式表示 image.png 假设用户对商品的喜好如下表所示 商品a 商品 b 商品 c 商品 d 用户A ✅ ✅ ✅ 用户B ✅ ✅ ✅ 用户C ✅ ✅ 那么根据上述的相似度计算公式可以得到ab 相似度为 0.71 ，ac 相似度为 0.58，bc 相似度为 0.82，cd 相似度为 0.82我们人为假定某个用户对商品 a 有购买行为兴趣度为 10，对 b 商品有点击行为兴趣度为 5，那么我们可以得到该用户对商品 c 的兴趣度为 10 _ 0.58 + 5 _ 0.82 &#x3D; 9.9，对 d 的兴趣度为 0 ，因此可以给该用户推荐商品 c image.png 双塔模型双塔模型是一种在推荐领域召回、粗排阶段被广泛使用的深度学习模型，其结构非常简单如下图所示 image.png 左侧是 User 塔，右侧是 Item 塔，可将特征拆分为两大类：用户相关特征（用户基本信息、群体统计属性以及行为过的 Item 序列等）与 Item 相关特征（Item 基本信息、属性信息等），原则上，Context 上下文特征可以放入用户侧塔。对于这两个塔本身，则是经典的 DNN 模型，从特征 OneHot 到特征 Embedding，再经过几层 MLP 隐层，两个塔分别输出用户 Embedding 和 Item Embedding 编码。训练阶段，User Embedding 和 Item Embedding 做内积或者 Cosine 相似度计算，使得用户和正例 Item 在 Embedding 空间更接近，和负例 Item 在 Embedding 空间距离拉远，损失函数则可用标准交叉熵损失。在线服务阶段，对于海量的 Item 集合，可以通过 Item 侧塔，离线将所有 Item 转化成 Embedding，并存储进 ANN 检索系统如 Faiss 以供查询。当一个用户进行请求时，将用户最新行为过的 Item 作为用户侧塔的输入，然后通过用户侧塔打出 User Embedding，从 Faiss 库里拉取相似性得分 Top K 的 Item，做为个性化召回结果这种模式。这样也可以实时地体现用户即时兴趣的变化，这是特征实时的角度，做起来相对简单。 image.png 本章小结目前算法侧通过离线的方式训练召回数据，存储到对应的数据源中，工程侧需要通过召回配置完成多路召回-去重-过滤-融合的操作，将召回结果送到下一阶段进行个性化排序，对整个推荐链路而言，召回要求快，排序要求准，所以对召回系统的要求是稳定低延迟。 image.png 召回工程海拍客推荐目前服务触达、首页、搜索激活页、支付成功页、 我的页面等多个场景，涵盖了购前、购中、购后等多个不同阶段。如上面所述，商品召回作为推荐的第一步在整个推荐流程中起到了举足轻重的作用，直接影响了返回物料的质量，而合理的召回工程架构也一定程度上影响业务和算法迭代的速度与质量。 召回初代架构由于业务的特殊性以及诸多历史遗留问题，在之前海拍客的推荐架构中各业务系统需要各自完成商品召回，再按需调用精排服务完成商品排序等后续操作，召回数据源又各有不同，如触达业务使用了 MySQL 作为召回数据源，推荐业务使用了 Redis 作为主要的召回数据源，所涉及到的架构大致如下图所示 image.png 架构局限性这种架构在业务的迭代中也逐渐出现各种弊端 职责不清晰， 从整个推荐域看，这种架构增加了业务系统复杂度使得业务系统过重，系统和系统之间架构职责不清晰 扩展性差，依赖各业务系统各自完成召回动作，而商品召回本身并不是一个简单动作，需要完成召回-过滤-融合等一系列动作。除此以外，数据源的新增变更、数据结构的变化带来的适配工作也是一个让人头疼的问题 召回配置难，这一点是对算法同学而言的，原有架构缺少一个统一的召回配置平台，AB 实验的进行和验证受到影响，妨碍算法迭代效率提升 稳定性难保障，对平台开发同学而言，日常需要监控依赖数据源、召回的各个阶段，如数据源平均 rt，各阶段平均 rt、超时率，召回整体的兜底率等指标，业务系统越多越不利于监控，系统的稳定性也会大打折扣 召回系统设计基于如上所述的诸多缺点，我们着手完成了推荐召回的服务切分，使得触达、首页推荐、购后等多个场景的推荐业务召回部分能够得到统一 主体设计一般来说，整个召回阶段主要完成多路召回-过滤-去重-融合几个步骤，大致的作用如下 多路召回，根据配置采用不同的召回策略从不同的数据源中获取物料，原则上多路召回尽可能多的返回用户可能有兴趣的物料，通常会根据每路召回的后验表现来设置配比。 去重，这一步主要是针对每一路召回而言，通常只是简单的根据商品 id 或者一些简单属性去重，避免因为数据源清洗问题导致的重复曝光。 过滤，通常会存在一些不同的过滤规则，如用户维度的曝光过滤、点击过滤、购买过滤等，也可能是基于风控规则的过滤，如卖家作弊等处罚、黄图恶心图等过滤。 融合，多路召回的物料根据需要进行合并，截断选取若干物料进入下一阶段，可以按照召回策略优先级融合，也可以是多路召回投票融合，也可以是通过物料的指标权重融合。 针对上述召回系统的几个步骤和特性，我们设计了如下的系统架构，整个召回系统大体上可以分为三层，召回配置层、召回引擎层、数据依赖层。 image.png 召回配置层 召回配置层主要面向算法同学，旨在让算法同学方便快捷地进行召回层配置进行 AB 实验和后续的结果验证，在实现上，主要借助 Disconf 作为配置中心，在引擎层做 AB 分流、召回配置解析等前置操作。 召回引擎层 召回引擎层采用模块化设计，通过召回配置实现模块和任务节点的动态化编排，同时通过插件化思想提供了异构数据源的支持，大大降低了新增数据源的成本。在稳定性保障上，由于采用了模块-任务节点的设计，能够很好地实现多维度的监控，如场景-模块维度的 rt 监控、失败率监控，场景-任务节点维度的超时率监控，场景维度的兜底率、无结果率监控等，同时配合钉钉告警实现问题的早发现早止血。 召回存储层 召回存储层主要面向异构数据源设计，前面提到因为历史原因现有的召回数据源结构和存储介质都存在差异以满足不同业务系统的诉求，因此在改造召回系统的过程中需要充分考虑数据存储的问题，如首页推荐等场景召回数据结构相对简单，需要满足低延迟诉求所以一直以来优先考虑 Redis, 又如触达算法召回数据结构复杂，包含属性多，数据量大用 MySQL 或者 MongoDB 更合适些，又如后续业务可能存在向量召回的场景，使用 ElasticSearch 或者调用 faiss 服务更合适，因此必须要考虑异构数据源的接入便捷性。 实现细节这一节主要介绍下召回引擎部分的一些实现细节以及踩过的一些坑，整个召回召回引擎调度如下图所示 image.png 模块化设计 如上述，整个召回过程中大体经过了多路召回-去重-过滤-融合等步骤。在设计上可以将各个步骤封装成独立的模块(module)，AB 分流获取门店对应的召回配置后，根据配置编排所需要的模块完成调度。而模块和模块之间又可能存在依赖关系，如在召回系统中各模块之间是串行的，而在一些系统中存在模块并发执行的需求，所以在设计之初设计了如下结构来做兼容，简单来说同一个列表内多个模块并发执行，不同列表的模块串行执行。 image.png 在模块内，抽象出任务节点(TaskNode)，如在多路召回模块中，每一路召回相当于一个任务节点，彼此并发执行，又比如在过滤模块中，每一种过滤策略可以当做一个任务节点获取待过滤数据。通过任务节点并发的方式可以有效降低 RT,相较于之前串行召回的方式平均减少 RT 约 16%。如下代码大致演示了模块内任务节点执行的实现。 image.png 1234567891011121314151617181920212223242526272829303132/** * 模块执行只需要执行任务列表中的任务即可 * 任务列表中的任务先暂时都并发执行 后面可以支持并发和顺序两种模式 * 即 [[A,B],[C]] A、B并发执行完成后再执行C * * @param requestContext 召回请求上下文 * @return recallModuleResultDto 召回模块结果对象 */@Overridepublic ModuleResult invoke(RequestContext requestContext) &#123; RecallStrategyConfig recallStrategyConfig = requestContext.getRecallStrategyConfig(); if (recallStrategyConfig == null) &#123; return null; &#125; ModuleResult moduleResult = new ModuleResult(); moduleResult.setTaskResultList(new ArrayList&lt;&gt;()); if (CollectionUtils.isEmpty(this.taskList)) &#123; return moduleResult; &#125; // 并发执行任务列表中的任务 使用arrayList保留任务原始的顺序 List&lt;Tuple&lt;TaskNode, CompletableFuture&lt;TaskResult&gt;&gt;&gt; tupleList = new ArrayList&lt;&gt;(); for (TaskNode taskNode : taskList) &#123; CompletableFuture&lt;TaskResult&gt; future = CompletableFuture.supplyAsync(() -&gt; taskNode.invoke(requestContext), ThreadUtil.executor); tupleList.add(new Tuple&lt;&gt;(taskNode, future)); &#125; // 结果获取 省略... return moduleResult;&#125; 模块内多个任务节点并发执行有一些需要关注的点，在实现中可能需要特别关注。 任务节点超时处理， 首先任务节点必须配置超时时间，避免因为某一个任务节点引起的服务雪崩，其次不同的任务节点其超时时间可以根据经验做配置化，如多路召回中 MongoDB 和 Redis 的超时时间可以根据监控做动态化调整，再比如对于无执行先后顺序要求的任务节点可以适当调整获取结果顺序以避免空结果等。 召回结果在不同阶段的传递，在目前的召回引擎实现中采用了上下文的方式传递召回中间结果，上下文中会保存不同阶段的模块结果(其中包含各阶段的召回结果)，保存不同阶段召回结果即每个阶段召回结果均为深拷贝以避免对上一个模块结果的破坏。 空结果处理， 在实际运行中可能存在各种情况导致召回结果为空，此时需要进行服务端兜底召回，除此以外当常规召回数量不够也需要使用兜底数据补召回，兜底召回数据请求实际也需要权衡，目前的实现中把兜底召回作为一路召回在多路并发召回阶段执行，通过多一次 IO 来降低服务的总 RT(兜底召回不一定被使用) 异构数据源支持 召回引擎提供了对多种数据源的支持，目前已支持如 Redis、MySQL、MongoDB、RPC 等多种数据源。以触达算法召回为例，数据量大且字段属性多，非常适合以 MongoDB 作为存储数据源，而首页推荐、购后推荐等场景的召回数据大多结构简单且要求召回速度快，因此以 Redis 作为存储数据源更合适。常规的单路召回需要经过获取数据-去重-截断-类型转换等步骤，其中获取数据和类型转换需要根据数据源类型做适配，因此在设计之初采用插件的方式支持异构数据源，只要实现几个简单方法就可以完成数据源的新增，如下代码大致展示了数据源插件的抽象定义 1234567891011121314151617public abstract class AbstractRecaller&lt;OriginType, ResultType&gt; implements Recaller&lt;OriginType, ResultType&gt; &#123; //部分代码省略 @Override public ResultType recall(RequestContext requestContext, RecallSourceConfig recallSourceConfig) &#123; // 获取数据源数据 OriginType originData = fetch(requestContext, recallSourceConfig); // 去重 originData = distinct(originData); // 截断 originData = cut(originData, recallSourceConfig.getLen()); // 转换 ResultType result = convert(originData, recallSourceConfig); return result; &#125;&#125; 以一个实际的单路召回为例，召回策略配置如下，表示该路召回策略存储数据源为 MongoDB，召回类型为 s2i，召回的主键标识为 actnew_sign,同时需要返回对应文档的指定字段 12345mongo_act_new_sign: dataSource: mongo fields: itemId,shopId,feature,rankNum,orderedDays key: act_new_sign_ type: recall_s2i 在运行的过程中召回引擎先会根据召回类型、绑定的数据源元信息等匹配对应的召回器 Recaller, 召回器的注册使用自定义注解@AiRecaller 完成，如下代码即为以 MongoDB 作为数据源的接入方式。 123456789101112131415161718192021222324252627282930@AiRecaller( name = \"touchMongoRecaller\", dataSourceType = RecallDataSourceType.mongo, recallTypes = &#123;RecallType.recall_s2i&#125;, dbName = \"ai_recall\", tableName = \"ai_touch\")public class TouchMongoRecaller extends AbstractRecaller&lt;List&lt;TouchRecallDo&gt;, TriggerResult&gt; &#123; @Resource private MongoTemplate mongoTemplate; @Override public List&lt;TouchRecallDo&gt; fetch(RequestContext requestContext, RecallSourceConfig recallSourceConfig) &#123; String sceneId = requestContext.getRecallReq().getSceneId(); String recallKey = recallSourceConfig.getKey(); String shopId = requestContext.getRecallReq().getShopId(); // 查询 List&lt;TouchRecallDo&gt; touchRecallDos = null; Query query = new Query(); // TODO 这边可以结合管理后台实现查询条件的配置化 query.addCriteria(Criteria.where(\"shop_id\").is(shopId)) .addCriteria(Criteria.where(\"recall_key\").is(recallKey)) .with(Sort.by(Sort.Order.asc(\"rank_num\"))); try &#123; touchRecallDos = mongoTemplate.find(query, TouchRecallDo.class); &#125; catch (Exception e) &#123; logger.error(\"[TouchMongoRecaller-fetch] sceneId:&#123;&#125; recallKey:&#123;&#125; shopId:&#123;&#125; exception cause:\", sceneId, recallKey, shopId, e); &#125; return touchRecallDos; &#125; 需要说明的是目前召回引擎多以离线方式计算落库，实时召回也多以用户实时 Query 等作为 trigger，但现有的召回系统架构可以快速支持如基于向量的实时召回等方式，这也是后续召回引擎一个迭代的方向。 总结和展望如上文提及，目前算法侧的召回数据均以离线方式产出落库，因此对实时特征的利用相对不足。后续召回引擎可以结合特征平台建设，尝试基于 ElasticSearch 或者 Faiss 的在线检索召回服务。 文献引用推荐系统技术演进趋势：从召回到排序再到重排SENet 双塔模型：在推荐领域召回粗排的应用及其它","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://xiaoysec.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"系统设计","slug":"系统设计","permalink":"https://xiaoysec.github.io/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"name":"召回","slug":"召回","permalink":"https://xiaoysec.github.io/tags/%E5%8F%AC%E5%9B%9E/"}]},{"title":"Slf4j源码浅析","date":"2019-05-30T17:03:50.000Z","path":"2019/05/31/Slf4j源码浅析/","text":"问题复现在项目中引入一个二方包后在后台日志诡异的不见了，于是使用 mvn dependency:tree -l tree.txt 输出依赖关系树，并定位到新引入的二方包部分,二方包引入了spring-boot-starter-logging其依赖了logback-classic与项目中的log4j产生了冲突，所以将前者排掉就项目就可以正常启动了，这个排包的过程不算难，那slf4j是如何实现绑定的呢？ slf4j源码浅析这里使用到的slf4j-api版本是1.7.25，在老版本中是存在一些线程安全问题的，通常我们打日志的时候都会写一段这样的代码 1private static final Logger logger = LoggerFactory.getLogger(XXX.class) 我们从LoggerFactory这个类开始分析源码,首先看到getLogger方法 12345678public static Logger getLogger(Class&lt;?&gt; clazz) &#123; Logger logger = getLogger(clazz.getName()); .....&#125;public static Logger getLogger(String name) &#123; ILoggerFactory iLoggerFactory = getILoggerFactory(); return iLoggerFactory.getLogger(name);&#125; 从上面源码可以看到会调用getLogger(String name)这个方法，通过LoggerFactory最终获取Logger对象，所以重点就是获取LoggerFactory. 我们重点看下 getILoggerFactory这个方法 1234567891011121314151617181920212223 public static ILoggerFactory getILoggerFactory() &#123; if (INITIALIZATION_STATE == UNINITIALIZED) &#123; synchronized (LoggerFactory.class) &#123; if (INITIALIZATION_STATE == UNINITIALIZED) &#123; INITIALIZATION_STATE = ONGOING_INITIALIZATION; performInitialization(); &#125; &#125; &#125; switch (INITIALIZATION_STATE) &#123; case SUCCESSFUL_INITIALIZATION: return StaticLoggerBinder.getSingleton().getLoggerFactory(); case NOP_FALLBACK_INITIALIZATION: return NOP_FALLBACK_FACTORY; case FAILED_INITIALIZATION: throw new IllegalStateException(UNSUCCESSFUL_INIT_MSG); case ONGOING_INITIALIZATION: // support re-entrant behavior. // See also http://jira.qos.ch/browse/SLF4J-97 return SUBST_FACTORY; &#125; throw new IllegalStateException(\"Unreachable code\");&#125; 从源码中可以看到INITIALIZATION_STATE是一个静态的volatile变量，在之前的版本中没有volatile修饰，在这个方法中主要就是调用了performInitialization方法完成初始化,在该方法中主要完成绑定工作并进行检查 123456789101112131415161718192021222324private final static void performInitialization() &#123; bind(); if (INITIALIZATION_STATE == SUCCESSFUL_INITIALIZATION) &#123; versionSanityCheck(); &#125; &#125; private final static void bind() &#123; try &#123; Set&lt;URL&gt; staticLoggerBinderPathSet = null; if (!isAndroid()) &#123; staticLoggerBinderPathSet = findPossibleStaticLoggerBinderPathSet(); reportMultipleBindingAmbiguity(staticLoggerBinderPathSet); &#125; // the next line does the binding StaticLoggerBinder.getSingleton(); INITIALIZATION_STATE = SUCCESSFUL_INITIALIZATION; reportActualBinding(staticLoggerBinderPathSet); fixSubstituteLoggers(); replayEvents(); // release all resources in SUBST_FACTORY SUBST_FACTORY.clear(); &#125; ... 其中有个重要的方法findPossibleStaticLoggerBinderPathSet 顾名思义就是找可能存在的StaticLoggerBinder路径 1234567891011121314151617181920212223private static String STATIC_LOGGER_BINDER_PATH = \"org/slf4j/impl/StaticLoggerBinder.class\"; static Set&lt;URL&gt; findPossibleStaticLoggerBinderPathSet() &#123; // use Set instead of list in order to deal with bug #138 // LinkedHashSet appropriate here because it preserves insertion order // during iteration Set&lt;URL&gt; staticLoggerBinderPathSet = new LinkedHashSet&lt;URL&gt;(); try &#123; ClassLoader loggerFactoryClassLoader = LoggerFactory.class.getClassLoader(); Enumeration&lt;URL&gt; paths; if (loggerFactoryClassLoader == null) &#123; paths = ClassLoader.getSystemResources(STATIC_LOGGER_BINDER_PATH); &#125; else &#123; paths = loggerFactoryClassLoader.getResources(STATIC_LOGGER_BINDER_PATH); &#125; while (paths.hasMoreElements()) &#123; URL path = paths.nextElement(); staticLoggerBinderPathSet.add(path); &#125; &#125; ... return staticLoggerBinderPathSet; &#125; 其实就是找日志实现包中的的StaticLoggerBinder，如slf4j-log4j12中的，将这些类的类路径添加到上面的set中，接着通过reportMultipleBindingAmbiguity方法检查是不是存在多个日志实现绑定产生冲突, 即看一下binderPathSet中元素个数是不是大于1，很简单，因此当项目中logback和slf4j-log4j同时存在时会打印出多个”Found binding in …” 12345678910111213private static boolean isAmbiguousStaticLoggerBinderPathSet(Set&lt;URL&gt; binderPathSet) &#123; return binderPathSet.size() &gt; 1; &#125; private static void reportMultipleBindingAmbiguity(Set&lt;URL&gt; binderPathSet) &#123; if (isAmbiguousStaticLoggerBinderPathSet(binderPathSet)) &#123; Util.report(\"Class path contains multiple SLF4J bindings.\"); for (URL path : binderPathSet) &#123; Util.report(\"Found binding in [\" + path + \"]\"); &#125; Util.report(\"See \" + MULTIPLE_BINDINGS_URL + \" for an explanation.\"); &#125; &#125; 回到bind方法，看到StaticLoggerBinder.getSingleton();，其实就是创建一个单例的StaticLoggerBinder对象，而这个对象中含有一个LoggerFactory，针对不同的日志框架有不同的实现 log4j1234567891011private final ILoggerFactory loggerFactory;private StaticLoggerBinder() &#123; loggerFactory = new Log4jLoggerFactory(); try &#123; Level level = Level.TRACE; &#125; catch (NoSuchFieldError nsfe) &#123; Util .report(\"This version of SLF4J requires log4j version 1.2.12 or later. See also http://www.slf4j.org/codes.html#log4j_version\"); &#125;&#125; logback-classic1234567private boolean initialized = false;private LoggerContext defaultLoggerContext = new LoggerContext();private final ContextSelectorStaticBinder contextSelectorBinder = ContextSelectorStaticBinder.getSingleton();private StaticLoggerBinder() &#123; defaultLoggerContext.setName(CoreConstants.DEFAULT_CONTEXT_NAME);&#125; 通过这种绑定的方式就可以实现LoggerFactory的获取，如引入了log4j，就会利用log4j实现的StaticLoggerBinder类来获取log4j的LoggerFactory,而LoggerFactory可以简单地理解为一个Map，key为loggerName,value为Logger对象","tags":[{"name":"Java","slug":"Java","permalink":"https://xiaoysec.github.io/tags/Java/"},{"name":"slf4j","slug":"slf4j","permalink":"https://xiaoysec.github.io/tags/slf4j/"}]}]